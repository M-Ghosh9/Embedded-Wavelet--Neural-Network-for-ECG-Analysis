{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c327b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd920add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    df = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "    df_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "    df_val_train = df.values\n",
    "    X = df_val_train[:, :-1]\n",
    "    y = df_val_train[:, -1].astype(int)\n",
    "    \n",
    "    df_val_test = df_test.values\n",
    "    X_test = df_val_test[:, :-1]\n",
    "    Y_test = df_val_test[:, -1].astype(int)\n",
    "\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_train, Y_train = ros.fit_resample(X, y)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "                                        X_train, \n",
    "                                        Y_train, \n",
    "                                        test_size=0.25, \n",
    "                                        random_state=42)\n",
    "    \n",
    "    shuffle_idx = np.random.permutation(list(range(X_train.shape[0])))\n",
    "    X_train = X_train[shuffle_idx]\n",
    "    Y_train = Y_train[shuffle_idx]\n",
    "    \n",
    "    X_train = np.expand_dims(X_train, 2)\n",
    "    X_val = np.expand_dims(X_val, 2)\n",
    "    X_test = np.expand_dims(X_test, 2)\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    Y_test = ohe.fit_transform(Y_test.reshape(-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366c1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Conv1D,ReLU,Add,MaxPooling1D,Flatten,Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def model_ecg():\n",
    "    input = Input(shape=(360, 1))\n",
    "    CONV1 = Conv1D(filters=32, kernel_size=5, strides=1)(input)\n",
    "    CONV1_1 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(CONV1)\n",
    "    ACT1_1 = ReLU()(CONV1_1)\n",
    "    CONV1_2 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(ACT1_1)\n",
    "    ADD1_1 = Add()([CONV1_2, CONV1])\n",
    "    ACT1_2 = ReLU()(ADD1_1)\n",
    "    MAX1_1 = MaxPooling1D(pool_size=5, strides=2)(ACT1_2)\n",
    "\n",
    "    CONV2_1 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(MAX1_1)\n",
    "    ACT2_1 = ReLU()(CONV2_1)\n",
    "    CONV2_2 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(ACT2_1)\n",
    "    ADD2_1 = Add()([CONV2_2, MAX1_1])\n",
    "    ACT2_2 = ReLU()(ADD2_1)\n",
    "    MAX2_1 = MaxPooling1D(pool_size=5, strides=2)(ACT2_2)\n",
    "\n",
    "    CONV3_1 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(MAX2_1)\n",
    "    ACT3_1 = ReLU()(CONV3_1)\n",
    "    CONV3_2 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(ACT3_1)\n",
    "    ADD3_1 = Add()([CONV3_2, MAX2_1])\n",
    "    ACT3_2 = ReLU()(ADD3_1)\n",
    "    MAX3_1 = MaxPooling1D(pool_size=5, strides=2)(ACT3_2)\n",
    "\n",
    "    CONV4_1 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(MAX3_1)\n",
    "    ACT4_1 = ReLU()(CONV4_1)\n",
    "    CONV4_2 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(ACT4_1)\n",
    "    ADD4_1 = Add()([CONV4_2, MAX3_1])\n",
    "    ACT4_2 = ReLU()(ADD4_1)\n",
    "    MAX4_1 = MaxPooling1D(pool_size=5, strides=2)(ACT4_2)\n",
    "\n",
    "    CONV5_1 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(MAX4_1)\n",
    "    ACT5_1 = ReLU()(CONV5_1)\n",
    "    CONV52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(ACT5_1)\n",
    "    ADD5_1 = Add()([CONV52, MAX4_1])\n",
    "    ACT5_2 = ReLU()(ADD5_1)\n",
    "    MAX5_1 = MaxPooling1D(pool_size=5, strides=2)(ACT5_2)\n",
    "\n",
    "    FLA1 = Flatten()(MAX5_1)\n",
    "    DEN1 = Dense(32)(FLA1)\n",
    "    ACT = ReLU()(DEN1)\n",
    "    ADD1 = Dense(5, activation=\"softmax\")(ACT)\n",
    "\n",
    "    model = Model(inputs=input, outputs=ADD1)\n",
    "\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.75,\n",
    "        staircase=True)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate=lr_schedule),\n",
    "        loss = SparseCategoricalCrossentropy(),\n",
    "        metrics = [\"sparse_categorical_accuracy\"]) \n",
    "    \n",
    "    return model_ecg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e39df",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fb55640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5e00b4703074>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-5e00b4703074>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                                 monitor='val_loss', save_best_only=False, verbose=0, period=1)]\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     model.fit( X_train, Y_train, \n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "#from model_ecg import model_ecg\n",
    "#from load import load_data\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "def main() -> object:\n",
    "    \n",
    "    model = model_ecg()\n",
    "    X_train, Y_train, X_val, Y_val, _, _ = load_data()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10, verbose=1),\n",
    "                ModelCheckpoint(filepath ='.{val_loss:.3f}-{val_sparse_categorical_accuracy:.3f}-{epoch:03d}-{loss:.3f}-{sparse_categorical_accuracy:.3f}.h5', \n",
    "                                monitor='val_loss', save_best_only=False, verbose=0, period=1)]\n",
    "    \n",
    "    model.fit( X_train, Y_train, \n",
    "            validation_data = (X_val, Y_val),\n",
    "            batch_size = 256,\n",
    "            epochs = 100, \n",
    "            callbacks = callbacks\n",
    "            )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88179cf",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4c34096",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b7fe9156542a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mload\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_avg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'load'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def evaluate_metrics(confusion_matrix, y_test, y_pred, print_result=False, f1_avg='macro'):\n",
    "    \n",
    "    TP = np.diag(confusion_matrix)\n",
    "    FP = confusion_matrix.sum(axis=0) - TP\n",
    "    FN = confusion_matrix.sum(axis=1) - TP    \n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    TPR = TP / (TP + FN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    PPV = TP / (TP + FP)\n",
    "\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "    ACC_macro = np.mean(ACC)  \n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average=f1_avg)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    if (print_result):\n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        print(\"============ METRICS ============\")\n",
    "        print(confusion_matrix)\n",
    "        print(\"Accuracy (macro) : \", ACC_macro)        \n",
    "        print(\"F1 score         : \", f1)\n",
    "        print(\"Cohen Kappa score: \", kappa)\n",
    "        print(\"======= Per class metrics =======\")\n",
    "        print(\"Accuracy         : \", ACC)\n",
    "        print(\"Sensitivity (TPR): \", TPR)\n",
    "        print(\"Specificity (TNR): \", TNR)\n",
    "        print(\"Precision (+P)   : \", PPV)\n",
    "    \n",
    "    return ACC_macro, ACC, TPR, TNR, PPV, f1, kappa\n",
    "\n",
    "def test_tf_model(X_test, y_test, model):\n",
    "    predictions, exec_times = [], []\n",
    "    segments = X_test.astype(np.float32)\n",
    "\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    for i in range(segments.shape[0]):\n",
    "        segment = segments[[i],:,:]\n",
    "        start_time = time.time()\n",
    "\n",
    "        pred = model.predict(segment, verbose=0)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        exec_times.append(end_time-start_time)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    predictions = np.vstack(predictions)\n",
    "    y_pred = np.argmax(predictions,1)\n",
    "\n",
    "    exec_times = np.array(exec_times) * 1000\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ACC_macro, ACC, TPR, TNR, PPV, f1, kappa = evaluate_metrics(cm, y_test, y_pred, True)\n",
    "\n",
    "    return y_pred, exec_times, (cm, ACC_macro, ACC, TPR, TNR, PPV, f1, kappa)    \n",
    "\n",
    "def load_test_data():\n",
    "    _, _, _, _, X_test, Y_test = load_data()\n",
    "    \n",
    "    ohe = OneHotEncoder()\n",
    "    Y_test = ohe.fit_transform(Y_test.reshape(-1,1))\n",
    "    \n",
    "    return X_test, Y_test\n",
    "\n",
    "def main(model_path) -> object:\n",
    "    model = load_model(model_path)\n",
    "    X_test, Y_test = load_test_data()\n",
    "    \n",
    "    y_pred_1, exec_times_1, cm = test_tf_model(X_test, Y_test, model)\n",
    "    print(\"Mean of execution times: \" , np.mean(exec_times_1))\n",
    "    print(\"Standard diviation: \", np.std(exec_times_1))\n",
    "    print(\"Maximum: \", np.max(exec_times_1))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Load model')\n",
    "    parser.add_argument('--model', metavar='path', required=True, help='path to model')\n",
    "    args = parser.parse_args()    \n",
    "    \n",
    "    main(model_path = args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99ce3748",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0e200b609a11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtflite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mload\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_avg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'load'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "def evaluate_metrics(confusion_matrix, y_test, y_pred, print_result=False, f1_avg='macro'):\n",
    "    \n",
    "    TP = np.diag(confusion_matrix)\n",
    "    FP = confusion_matrix.sum(axis=0) - TP\n",
    "    FN = confusion_matrix.sum(axis=1) - TP    \n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "    TPR = TP / (TP + FN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    PPV = TP / (TP + FP)\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "    ACC_macro = np.mean(ACC)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average=f1_avg)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    if (print_result):\n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        print(\"============ METRICS ============\")\n",
    "        print(confusion_matrix)\n",
    "        print(\"Accuracy (macro) : \", ACC_macro)        \n",
    "        print(\"F1 score         : \", f1)\n",
    "        print(\"Cohen Kappa score: \", kappa)\n",
    "        print(\"======= Per class metrics =======\")\n",
    "        print(\"Accuracy         : \", ACC)\n",
    "        print(\"Sensitivity (TPR): \", TPR)\n",
    "        print(\"Specificity (TNR): \", TNR)\n",
    "        print(\"Precision (+P)   : \", PPV)\n",
    "    \n",
    "    return ACC_macro, ACC, TPR, TNR, PPV, f1, kappa\n",
    "\n",
    "\n",
    "def load_lite_model(lite_model=None, tflite_load='Loaded', filename=None):\n",
    "    if tflite_load == 'Loaded':\n",
    "        interpreter = tflite.Interpreter(model_content=lite_model)\n",
    "    elif tflite_load == 'File':\n",
    "        interpreter = tflite.Interpreter(model_path=filename + \".tflite\")\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    return interpreter, input_index, output_index\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    _, _, _, _, X_test, Y_test = load_data()\n",
    "    return X_test, Y_test\n",
    "\n",
    "\n",
    "def test_lite_model(X_test, y_test, interpreter, input_index, output_index):\n",
    "    predictions = []\n",
    "    exec_times  = []\n",
    "    segments = X_test.astype(np.float32)\n",
    "\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    for i in range(segments.shape[0]):\n",
    "        segment = segments[[i],:,:]\n",
    "        start_time = time.time()\n",
    "        interpreter.set_tensor(input_index, segment)\n",
    "        interpreter.invoke()\n",
    "        pred = interpreter.get_tensor(output_index)\n",
    "        end_time = time.time()\n",
    "        exec_times.append(end_time-start_time)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    predictions = np.vstack(predictions)\n",
    "    y_pred = np.argmax(predictions,1)\n",
    "\n",
    "    exec_times = np.array(exec_times) * 1000\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ACC_macro, ACC, TPR, TNR, PPV, f1, kappa = evaluate_metrics(cm, y_test, y_pred, True)\n",
    "\n",
    "    return y_pred, exec_times, (cm, ACC_macro, ACC, TPR, TNR, PPV, f1, kappa) \n",
    "\n",
    "def main() -> object:\n",
    "    X_test, Y_test = load_test_data()\n",
    "\n",
    "    FILENAME = 'models/ecg_quant'\n",
    "    interpreter_1, input_idx_1, output_idx_1 = load_lite_model(tflite_load='File', filename=FILENAME)\n",
    "    y_pred, exec_times, cm = test_lite_model(X_test, Y_test, interpreter_1, input_idx_1, output_idx_1)\n",
    "\n",
    "    print(\"Mean of execution times: \" , np.mean(exec_times))\n",
    "    print(\"Standard diviation: \", np.std(exec_times))\n",
    "    print(\"Maximum: \", np.max(exec_times))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda51bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
